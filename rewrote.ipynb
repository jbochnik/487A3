{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import re\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PA3\") \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and training data\n",
    "df_list = df.randomSplit([.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test and train data\n",
    "train_data= df_list[0]\n",
    "test_data = df_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data \n",
    "train_data =train_data.withColumn('plot',lower(regexp_replace(train_data['plot'],\"[^a-zA-Z ]\",\"\")))\n",
    "test_data =test_data.withColumn('plot',lower(regexp_replace(test_data['plot'],\"[^a-zA-Z ]\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of stop words\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#routine to remove stop words and count all words in test data to find common words to use as features\n",
    "find_common = train_data.select('plot')\n",
    "find_common = find_common.rdd.map(list)\n",
    "find_common = find_common.map(lambda x:x[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x):\n",
    "    to_remove = []\n",
    "    for word in x:\n",
    "        if word not in stop:\n",
    "            continue\n",
    "        else:\n",
    "            to_remove.append(word)\n",
    "    for word in to_remove:\n",
    "        x.remove(word)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_common = find_common.map(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_common = find_common.flatMap(lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_common = find_common.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_common = find_common.reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = find_common.filter(lambda x: x[1] > 200 and x[1] < 91000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = common_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now create features for ML algo using the common words as the features as sparse vectors for each plot\n",
    "from pyspark .mllib.linalg import SparseVector\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uncommon_words(x):\n",
    "    for word in x:\n",
    "        if word in features:\n",
    "            continue\n",
    "        else:\n",
    "            x.remove(word)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = common_words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get plots for train and test data\n",
    "tr_data = train_data.select('plot')\n",
    "te_data = test_data.select('plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize plot into a list of lists\n",
    "tr_data = tr_data.rdd.flatMap(lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split on space to get list of words\n",
    "tr_data = tr_data.map(lambda x : x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data = te_data.rdd.flatMap(lambda x:x)\n",
    "te_data = te_data.map(lambda x : x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reomve uncommon words\n",
    "tr_data = tr_data.map(remove_uncommon_words)\n",
    "te_data = te_data.map(remove_uncommon_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = tr_data.map(remove_stop_words)\n",
    "te_data = te_data.map(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = tr_data.flatMap(lambda token: token).distinct() \\\n",
    "    .zipWithIndex().collectAsMap()\n",
    "v_map = sc.broadcast(vocab_map)\n",
    "vocab_size = sc.broadcast(len(vocab_map))\n",
    "tdm_train = tr_data \\\n",
    "    .map(Counter) \\\n",
    "    .map(lambda counts: {v_map.value[token]: float(counts[token]) for token in counts})\\\n",
    "    .map(lambda index_counts: SparseVector(vocab_size.value, index_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map_test = te_data.flatMap(lambda token: token).distinct() \\\n",
    "    .zipWithIndex().collectAsMap()\n",
    "v_map_test = sc.broadcast(vocab_map_test)\n",
    "vocab_size_test = sc.broadcast(len(vocab_map_test))\n",
    "tdm_test = te_data \\\n",
    "    .map(Counter) \\\n",
    "    .map(lambda counts: {v_map_test.value[token]: float(counts[token]) for token in counts})\\\n",
    "    .map(lambda index_counts: SparseVector(vocab_size_test.value, index_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data.select('genre')\n",
    "train_labels= train_labels.rdd.flatMap(lambda x:x)\n",
    "test_labels = test_data.select('genre')\n",
    "test_labelslabels= test_labels.rdd.flatMap(lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseVectorsList = tdm_train.collect()\n",
    "sparseVectorsList_Test = tdm_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movie_ids = train_data.select('movie_id')\n",
    "train_movie_ids = train_movie_ids.rdd\n",
    "train_movie_ids = train_movie_ids.flatMap(lambda x:x)\n",
    "train_movie_ids = train_movie_ids.collect()\n",
    "\n",
    "test_movie_ids = test_data.select('movie_id')\n",
    "test_movie_ids = test_movie_ids.rdd\n",
    "test_movie_ids = test_movie_ids.flatMap(lambda x:x)\n",
    "test_movie_ids = test_movie_ids.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = pd.read_csv('mapping.csv')\n",
    "keylist.rename(columns={'Unnamed: 0': 'i'}, inplace=True)\n",
    "keylist = spark.createDataFrame(keylist)\n",
    "k = keylist.select('0')\n",
    "k1 = keylist.select('i')\n",
    "k = k.rdd.flatMap(lambda x:x)\n",
    "k1 = k1.rdd.flatMap(lambda x:x)\n",
    "genre_list = k1.zip(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data):\n",
    "    ans = []\n",
    "    for i in range(len(data)):\n",
    "        val = 0.0\n",
    "        labels = data[i][0].replace('[','').replace(']','').replace('\\'','').split(',')\n",
    "        for lab in labels:\n",
    "            if lab == label:\n",
    "                val = 1.0\n",
    "                break\n",
    "        pt = LabeledPoint(val, data[i][1])\n",
    "        ans.append(pt)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeClassifier(data):\n",
    "    lst = test(data)\n",
    "    return SVMWithSGD.train(sc.parallelize(lst), iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2sparse = train_labels.zip(tdm_train)\n",
    "lab2sparse_test = test_labels.rdd.zip(tdm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2sm = lab2sparse.map(lambda x: (x[0],x[1]))\n",
    "data = l2sm.collect()\n",
    "l2sm_test =lab2sparse_test.map(lambda x: (x[0],x[1]))\n",
    "data_test = l2sm_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "\n",
    "for label in k.collect():\n",
    "    print('Trained', label, 'Classifier')\n",
    "    classifiers.append(makeClassifier(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
