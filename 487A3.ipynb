{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PA3\") \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test =  pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "df_test = spark.createDataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =df.withColumn('plot',lower(regexp_replace(df['plot'],\"[^a-zA-Z ]\",\"\")))\n",
    "df1_test =df_test.withColumn('plot',lower(regexp_replace(df_test['plot'],\"[^a-zA-Z ]\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna()\n",
    "df1_test = df1_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.select('plot')\n",
    "df2_test = df1_test.select('plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rd = df2.rdd.flatMap(list)\n",
    "rd_test = df2_test.rdd.flatMap(list)\n",
    "data = df2.rdd.map(list)\n",
    "data_test = df2_test.rdd.map(list)\n",
    "data = data.map(lambda x: x[0].split())\n",
    "data_test = data_test.map(lambda x: x[0].split())\n",
    "def remove_stop_words(x):\n",
    "    to_remove = []\n",
    "    for word in x:\n",
    "        if word not in stop:\n",
    "            continue\n",
    "        else:\n",
    "            to_remove.append(word)\n",
    "    for word in to_remove:\n",
    "        x.remove(word)\n",
    "    return x\n",
    "data = data.map(remove_stop_words)\n",
    "data_test = data_test.map(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd1 = rd.flatMap(lambda x: x.split())\n",
    "rd1_test = rd_test.flatMap(lambda x:x.split())\n",
    "rd1 = rd1.filter(lambda x: x not in stop)\n",
    "rd1_test = rd1_test.filter(lambda x: x not in stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rd1.map(lambda x: (x,1))\n",
    "m_test = rd1_test.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = m.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = wordCount.filter(lambda x: x[1] > 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = common_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark .mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def remove_uncommon_words(x):\n",
    "    for word in x:\n",
    "        if word in features:\n",
    "            continue\n",
    "        else:\n",
    "            x.remove(word)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= features.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(remove_uncommon_words)\n",
    "data_test = data_test.map(remove_uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = data.flatMap(lambda token: token).distinct() \\\n",
    "    .zipWithIndex().collectAsMap()\n",
    "\n",
    "vocab_map_test = data_test.flatMap(lambda token: token).distinct() \\\n",
    "    .zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_map = sc.broadcast(vocab_map)\n",
    "vocab_size = sc.broadcast(len(vocab_map))\n",
    "tdm = data \\\n",
    "    .map(Counter) \\\n",
    "    .map(lambda counts: {v_map.value[token]: float(counts[token]) for token in counts})\\\n",
    "    .map(lambda index_counts: SparseVector(vocab_size.value, index_counts))\n",
    "\n",
    "\n",
    "v_map_test = sc.broadcast(vocab_map_test)\n",
    "vocab_size_test = sc.broadcast(len(vocab_map_test))\n",
    "tdm_test = data_test \\\n",
    "    .map(Counter) \\\n",
    "    .map(lambda counts: {v_map_test.value[token]: float(counts[token]) for token in counts})\\\n",
    "    .map(lambda index_counts: SparseVector(vocab_size_test.value, index_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df1.select('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= labels.rdd.flatMap(lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseVectorsList = tdm.collect()\n",
    "sparseVectorsList_Test = tdm_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = df.select('movie_id')\n",
    "movie_ids = movie_ids.rdd\n",
    "movie_ids = movie_ids.flatMap(lambda x:x)\n",
    "movie_ids = movie_ids.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = pd.read_csv('mapping.csv')\n",
    "keylist.rename(columns={'Unnamed: 0': 'i'}, inplace=True)\n",
    "keylist = spark.createDataFrame(keylist)\n",
    "k = keylist.select('0')\n",
    "k1 = keylist.select('i')\n",
    "k = k.rdd.flatMap(lambda x:x)\n",
    "k1 = k1.rdd.flatMap(lambda x:x)\n",
    "genre_list = k1.zip(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_list.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mapFunc(line):\n",
    "#     val = 0.0\n",
    "#     vals = line.map(lambda x: (x[0], x[1]))\n",
    "#     for lab in vals[0]:\n",
    "#         if label == lab:\n",
    "#             val = 1.0\n",
    "#             break\n",
    "#     return LabeledPoint(val, vals[1])\n",
    "\n",
    "def test(data):\n",
    "    ans = []\n",
    "    for i in range(len(data)):\n",
    "        val = 0.0\n",
    "        labels = data[i][0].replace('[','').replace(']','').replace('\\'','').split(',')\n",
    "        for lab in labels:\n",
    "            if lab == label:\n",
    "                val = 1.0\n",
    "                break\n",
    "        pt = LabeledPoint(val, data[i][1])\n",
    "        ans.append(pt)\n",
    "    return ans\n",
    "\n",
    "def makeClassifier(data):\n",
    "#     lab2Data = data.map(mapFunc)\n",
    "    lst = test(data)\n",
    "    return SVMWithSGD.train(sc.parallelize(lst), iterations=100)\n",
    "#     l2dl = lab2Data.collect()\n",
    "#     return SVMWithSGD.train(sc.parallelize(l2dl), iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2sparse = labels.zip(tdm)\n",
    "lab2sparse_test = labels_test.zip(tdm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2sparse_test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2sm = lab2sparse.map(lambda x: (x[0],x[1]))\n",
    "data = l2sm.collect()\n",
    "l2sm_test =lab2sparse_test.map(lambda x: (x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = l2sm_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "\n",
    "for label in k.collect():\n",
    "    print('Trained', label, 'Classifier')\n",
    "    classifiers.append(makeClassifier(data))\n",
    "# classifier = makeClassifier(lab2sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
